{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEcWmA6tuhRghPeZ+NoVf/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 신경망"],"metadata":{"id":"-fVdtgnz67aY"}},{"cell_type":"markdown","source":["### 1-1. 인공지능, 머신러닝, 딥러닝의 포함 관계\n","\n","<img src='https://tensorflowkorea.files.wordpress.com/2018/12/028.jpg'>\n","\n","* AI > 머신러닝 (SVM, 선형회귀, 로지스틱 회귀, 퍼셉트론, ...), 규칙기반 > 신경망 (DNM, RNN, CNN, GAN ..)\n","* 신경망은 머신러닝 알고리즘 중 하나\n"],"metadata":{"id":"Eqbg64-DMG5V"}},{"cell_type":"markdown","source":["### 1-2. 신경망의 특징\n","* 인간의 뇌기능을 흉내 내려고 만들어짐\n","* 하나하나의 뉴런들이 이어져서 신경을 전달\n","* 이와같이, 입력층 > 은닉층 > 출력층 으로 나누어져 있음\n","* 유닛들이 서로 연결된 것이고, 그 유닛들 사이의 가중치를 학습하기 위해 사용\n","* 층을 점점 늘려서 깊게 만든 신경망을 심층 신경망(Deep Neural Network)라고 부름\n","* 깊은 층을 가진 신경망의 가중치를 학습시키는 것을 딥러닝 또는 심층학습이라고 함"],"metadata":{"id":"cDf0dQqXM1r-"}},{"cell_type":"markdown","source":["### 1-3. 신경망으로 할 수 있는 것\n","* 회귀\n","* 분류\n","* 이미지 생성"],"metadata":{"id":"tgx7nrO0N6I-"}},{"cell_type":"markdown","source":["#2. 신경망의 진화"],"metadata":{"id":"1E2suRUjOByR"}},{"cell_type":"markdown","source":["* 1950년대 퍼셉트론(기계가 사람처럼 행동해 줄 수 있을 것 같다)에 대한 아이디어 나옴\n","* 1980년대 오차역전파법 신경망 학습을 개발 -> 데이터 부족으로 문제가 발생\n","* 2000년대 인터넷이 보급된 후 많은 데이터를 얻을 수 있게 되면서 다시 주목 받게됨\n","* 2012년\n","  * CNN중 Alex Net이라는 네트워크 구조\n","  * Alex net이 이미지 분류 대회에서 1등 차지 -> CNN이 유명해지게 됨\n","* 2013년\n","  * 아타리 게임 중 '벽돌깨기' -> AI 적용 -> 학습을 많이 진행 (강화학습) -> 구석을 파서 공을 위로 올리는 방법을 깨달음\n","  * 딥마인드 -> 구글에 인수 -> 알파고\n","* 2014년\n","  * RNN이라는 네트워크를 사용하여 중국어를 영어로 번역\n","  * 성능의 한계가 생김 (숙어 같은 것들로 인해)\n","  * attention 모델의 출현으로 성능이 급격히 좋아짐\n","* 2015년\n","  * GANs -> 오바마 합성영상\n","  * ResNet(Residual Networks): CNN의 종류중 하나. 사람과 AI가 이미지를 찾는 테스트를 함\n","    * 사람 오차율: 5%\n","    * ResNet의 오차율: 3%\n","* 2016년\n","  * 알파고 vs 이세돌 9단의 바둑\n","  * 컴퓨터를 이긴 마지막 인류\n","* 2017년\n","  * RNN의 단점을 극복한 attention만으로 만든 모델 -> Transformer\n","  * Transformer: 번역 모델\n","  * 자연어를 접목\n","  * Transformer는 딥러닝에서 가장 중요한 모델 중 하나\n","* 2018년\n","  * Transformer에서 인코더만 따온 모델 -> BERT\n","  * 자연어를 정답없이 사용 (인터넷 문장들의 데이터를 삽입)\n","  * 일부 글자를 가리고 가린 부분을 맟힐 수 있도록 학습\n","  * 모델을 크게 만들고 엄청난 데이터를 사용\n","* 2019년 ~\n","  * GPU 수천수만대를 사용하여 큰 모델을 만들고 학습\n","  * 정답이 필요없이 실제 데이터만으로 셀프 수퍼바이즈러닝"],"metadata":{"id":"35VPrqMbOV38"}},{"cell_type":"markdown","source":["# 3. 딥러닝의 키포인트"],"metadata":{"id":"04FY8Ab9PV3S"}},{"cell_type":"markdown","source":["* 데이터 (양질, 많은 양)\n","* 모델 (CNN, RNN, Transformer, multilayer perceptron, ...)\n","  * 모델을 직접 만들기 보단, 그 모델을 자기 것에 맞게 수정하여 사용\n","* 알고리즘 (Gradient Descent를 기초로 많은 알고리즘이 만들어짐)\n","* Loss Function, Cost Function: 모델의 성능이 얼마나 좋은지 척도\n"],"metadata":{"id":"HkMubIyxRUCg"}},{"cell_type":"markdown","source":[],"metadata":{"id":"S8Q8GYKNSh6c"}}]}