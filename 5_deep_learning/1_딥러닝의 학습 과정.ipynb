{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTr7CfihQXaV/KJMT0c/tR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 퍼셉트론\n","* 4개월차 9회차 30:00\n","* 가중치수 : 층사이와 유닛을 있는 number of 선\n","* 편향수 : 유닛의 갯수"],"metadata":{"id":"yrkdQ-hsdHdM"}},{"cell_type":"markdown","source":["# 1. 순전파 forward propagaion\n","* 순전파는 입력받은 데이터를 각 가중치와 곱하여 출력을 뽑아내는 과정\n","* 입력층부터 출력층 n층까지 차례대로 계산해가며 출력층까지 나아감"],"metadata":{"id":"vDgaIjKMfbVm"}},{"cell_type":"markdown","source":["# 2. 학습과정\n","1. 주어진 입력값에 상관없이 임의 초기 가중치를 준 뒤 은닉층을 거쳐 결과를 계산 (순전파)\n","2. 계산 결과와 실제 예측하고자 하는 값 사이의 오차를 구함 (loss function)\n","3.  계산식의 반대로 돌아가며(미분) 가중치를 업데이트 (역전파)\n","4. 1-3의 과정을 오차가 더이상 줄지 않을 때까지 반복 (에폭)"],"metadata":{"id":"4H9zj6qOmEUp"}},{"cell_type":"markdown","source":["### 2-1 활성화 함수 activation function a(x)\n","* Activation function 이라고 부르며 신경망의 출력을 결정하는 식\n","* 엑티베이션 함수가 아무리 많더라도, matrix multiplication을 거치다보면 xi는 하나의 일차방적식이 됨\n","  * a(xi(j), if x >= ?) = xi(j)\n","  * xi(j) = a(xi(j), ..)\n","  * xi = y 출력값\n","* 활성화 함수를 사용하지 않는다는 것은 f(x) = x 형태의 함수를 사용하는 것과 마찬가지임 (f(x) = x, f(x) = 2x, f(x) = 1/3x)"],"metadata":{"id":"fHv365WTnC97"}},{"cell_type":"markdown","source":["### 2-2. 활성화 함수의 종류"],"metadata":{"id":"3PyypojHqEhW"}},{"cell_type":"markdown","source":["1. Binary step function\n","  * 퍼셉트론 알고리즘에서 활성화 함수로 사용\n","  * 다중 분류 문제에서 다중 출력을 할 수 없다는 단점"],"metadata":{"id":"0zi0FtBMd6i0"}},{"cell_type":"markdown","source":["2. Linear activation function\n","  * 선형 활성화 함수\n","  * 입력값에 특정 상수값을 곱한 값을 출력으로 가짐\n","  * 역전파에 사용이 불가능\n","  * 다중 출력이 가능\n","  * 선형 함수를 여러번 사용하는 것은 마지막에 선형함수를 한번 쓰는 것과 같음"],"metadata":{"id":"gayFQbameCzX"}},{"cell_type":"markdown","source":["3. non-linear activation function\n","  * 최근 신경망 모델에서 대부분 사용\n","  * 입력과 출력간의 복잡한 관계를 만들어 입력에서 필요한 정보를 얻음\n","  * 입력과 관련있는 미분값을 얻으며 역전파를 가능"],"metadata":{"id":"KJvJICaHeQ09"}},{"cell_type":"markdown","source":["### 2-3. Non-linear activation function의 종류\n"],"metadata":{"id":"7GOg-FI3ebv3"}},{"cell_type":"markdown","source":["1. Sigmoid\n","  * 로지스틱으로도 불리는 s자 형태의 함수\n","  * 입력값이 커질수록 1로 수렴하고, 입력값이 작을수록 0에 수렴\n","  * 출력의 중심이 0이 아님\n","  * 비용/효율면에서 좋지 못함\n","  * 퍼셉트론 초기 신경망에서 많이 사용했지만 여러 단점들 때문에 현재는 많이 사용하지 않음\n","  * 역전파시 vanishing gradient 문제가 발생"],"metadata":{"id":"nkVCKoFKek5P"}},{"cell_type":"markdown","source":["2. ReLU\n","  * Rectified Linear Unit 함수의 준말로 개선 선형 함수라고 부름\n","  * CNN에서 좋은 성능을 보였고, 현재 딥러닝에서 가장 많이 사용하는 활성화 함수\n","  * 비선형, 도함수를 가지며, 역전파를 허용\n","  * 입력값이 0 또는 음수일 때, gradient값은 0이 되며 이 경우 학습을 하지 않음"],"metadata":{"id":"SchHnOb4jYCG"}},{"cell_type":"markdown","source":["3. softmax\n","  * 다중 분류 문제를 해결하기 위해 많이 사용되는 함수\n","  * 입력 받은 값을 0에서 1 사이의 값으로 모두 정규화하며 출력값이 여러개임\n","  * 출력값의 총합은 항상 1이 되는 특징을 가짐\n","  * 지수함수를 사용하여 오버플로 발생이 가능"],"metadata":{"id":"lbnS2xrOjsjG"}},{"cell_type":"markdown","source":["# 3. 역전파 back propagation"],"metadata":{"id":"FthjIUQmj6-7"}},{"cell_type":"markdown","source":["* 순전파의 반대방향으로 전파시키는 방법\n","* 출력층으로부터 입력층까지 역으로 나감"],"metadata":{"id":"Mo94nBZlkJfz"}},{"cell_type":"markdown","source":["### 3-1. 경사하강법 Gradient Descent\n","* 딥러닝 알고리즘 학습시 사용되는 최적화 방법 (optimizer)\n"],"metadata":{"id":"KB0F3ldOkjSe"}},{"cell_type":"markdown","source":["### 3-2 경사하강법의 종류\n"],"metadata":{"id":"1ZR4z4rPksns"}},{"cell_type":"markdown","source":["1. 배치 경사 하강법 Batch Gradient Descent\n","  * 가장 기본적인 경사 하강법\n","  * 데이터셋 전체를 고려하여 손실함수를 계산\n","  * 한번의 Epoch에 모든 파라미터 업데이트를 단 한번만 수행\n","  * 모델 학습시 많은 시간과 메모리가 필요하다는 단점"],"metadata":{"id":"BcF_9jZWkv3P"}},{"cell_type":"markdown","source":["2. 확률적 경사 하강법 Stochastic Gradient Descent\n","  * 배치 경사 하강법의 단점을 개선하기 위해 제안\n","  * batch size를 1로 설정하여 파라미터를 업데이트 하기 때문에 훨씬 빠르고 적은 메모리로 학습이 진행\n","  * 파라미터 값의 업데이트 폭이 불안정하기 때문에 정확도가 낮은 경우가 생길 수도 있음\n","  "],"metadata":{"id":"lUgpKEbZlHmz"}},{"cell_type":"markdown","source":["3. 미니 배치 경사 하강법 Mini-batch Gradient Descent\n","  * 일반적으로 배치사이즈를 32, 64, 128, ... 과 같이 2의 n제곱에 해당하는 값으로 설정하고 적용하는 방법\n","  * 모델 학습 속도가 빠르고, 확률적 경사 하강법보다 안정적인 장점\n","  "],"metadata":{"id":"BqvoYFEsl3-Z"}},{"cell_type":"markdown","source":["4. Mementum\n","  * 운동량을 의미하며 매개변수의 이동에 속도를 부여하는 것을 의미\n","  * 매개변수를 갱신할 때마다 기존 속도에 매개변수의 기울기 값을 추가하고 이 속도값을 다시 매개변수에 추가하여 매개변수를 이동\n","  * SGD가 가지고 있는 문제인 Local Minima를 해결할 수 있는 방법"],"metadata":{"id":"Mv06tKrTl8nf"}},{"cell_type":"markdown","source":["5. AdaGrad\n","  * Adaptive Gradient의 줄임말\n","  * 손실 함수 곡면의 변화에 따라 적응적으로 학습률을 정하는 알고리즘\n","  * 손실 함수 경사가 가파를 때 큰 폭으로 이동하여 최적화 경로를 벗어나서 최소 지점을 지나갈 수 있음\n","  * 많이 변환한 변수는 최적해에 근접했을거란 가정하에 작은 크기로 이동하면서 세밀하게 값을 조정하고, 반대로 적게 변환한 변수들은 학습률을 크게하여 빠르게 오차값을 줄이고자 하는 방법"],"metadata":{"id":"9Yt-RXe3mnwt"}},{"cell_type":"markdown","source":["6. RMSProp\n","  * AdaGrad에서 학습이 안되는 문제를 해결하기 위해 hyper parameter를 추가\n","  * 변화량이 클수록 학습률이 작아져서 조기 종료되는 문제를 해결하기 위해 학습률 크기를 비율로 조절할 수 있는 파라미터를 추가"],"metadata":{"id":"JZgljdDyoYGE"}},{"cell_type":"markdown","source":["7. Adam\n","  * Momentum과 RMSProp이 합쳐진 형태\n","  * 매우 넓은 범위의 아키텍처를 가진 서로 다른 신경망에서 잘 작동하다는 것이 증명되었기 때문에 현재 가장 많이 사용되는 알고리즘\n","  \n"],"metadata":{"id":"klIFpYNrK_MG"}},{"cell_type":"markdown","source":["> 2, 3, 7번 주목"],"metadata":{"id":"T474NO_fU4zp"}},{"cell_type":"code","source":[],"metadata":{"id":"EnoMps1vU7V5"},"execution_count":null,"outputs":[]}]}